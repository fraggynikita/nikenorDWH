import logging
import duckdb
from airflow import DAG
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from airflow.models import Variable
import requests
from datetime import datetime, timedelta
import pandas as pd


OWNER = "nikenor"
DAG_ID = "get_data_and_load_to_s3"

ACCESS_KEY = Variable.get("access_key")
SECRET_KEY = Variable.get("secret_key")

LAYER = "raw"
SOURCE = "news"

MINIO_BUCKET = "prod"
MINIO_ENDPOINT = "minio:9000"

API_KEY = Variable.get("api_key")  

args = {
    "owner": OWNER,
    "start_date": datetime(2025, 6, 10),
    "catchup": True,
    "retries": 3,
    "retry_delay": timedelta(minutes=20)
}

def get_dates(**kwargs) -> tuple[str, str]:
    """
    ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ start_date Ğ¸ end_date Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸
    """
    start_date = kwargs["data_interval_start"] - timedelta(days=2)
    end_date = kwargs["data_interval_end"] - timedelta(days=2)
    
    return start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")

def from_api_to_s3(**kwargs):
    """
    ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· API Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸Ñ… Ğ² S3.
    """
    start_date, end_date = get_dates(**kwargs)

    logging.info(f"ğŸ’» Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ·Ğ°: {start_date}/{end_date}")

    url = f"https://newsapi.org/v2/everything?q=*&from={start_date}&to={end_date}&language=en&sortBy=popularity&apiKey={API_KEY}"

    logging.info(f"ĞÑ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğº API Ğ¿Ğ¾ Ğ°Ğ´Ñ€ĞµÑÑƒ: {url}")
    response = requests.get(url)

    if response.status_code == 200:
        data = response.json()
        logging.info(f"ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ API.")

        if "articles" in data and len(data["articles"]) > 0:
            df = pd.DataFrame(data["articles"])

            logging.info(f"ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¾ {len(df)} Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹.")

            try:
                
                con = duckdb.connect()

                con.sql(f"""
                    SET TIMEZONE='UTC';
                    INSTALL httpfs;
                    LOAD httpfs;
                    SET s3_url_style = 'path';
                    SET s3_endpoint = '{MINIO_ENDPOINT}';
                    SET s3_access_key_id = '{ACCESS_KEY}';
                    SET s3_secret_access_key = '{SECRET_KEY}';
                    SET s3_use_ssl = FALSE;
                """)

                save_path = f"s3://{MINIO_BUCKET}/{LAYER}/{SOURCE}/{start_date}/{end_date}_news_data.parquet"
                con.from_df(df).to_parquet(save_path)

                logging.info(f"Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ğ² S3: {save_path}")

            except Exception as e:
                logging.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ DuckDB: {e}")
                raise
        else:
            logging.error("API Ğ½Ğµ Ğ²ĞµÑ€Ğ½ÑƒĞ»Ğ¾ Ğ¿Ğ¾Ğ»Ğµ 'articles' Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ÑƒÑÑ‚Ñ‹.")
            raise
    else:
        logging.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ API: {response.status_code} - {response.text}")
        raise

with DAG(
    dag_id=DAG_ID,
    schedule="0 9 * * *",  
    default_args=args,
    concurrency=1,
    max_active_tasks=1,
    max_active_runs=1,
) as dag:

    start = EmptyOperator(
        task_id="start",
    )

    from_api_to_s3_task = PythonOperator(
        task_id="from_api_to_s3_task",
        python_callable=from_api_to_s3,
    )

    end = EmptyOperator(
        task_id="end",
    )

    start >> from_api_to_s3_task >> end
